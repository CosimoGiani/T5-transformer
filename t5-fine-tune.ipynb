{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"t5-fine-tune.ipynb","provenance":[],"authorship_tag":"ABX9TyNWn4FTS/1VKDDJVRjK3OHI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## T5 fine-tuning \n","Here's the implementation for fine-tuning the desidered T5 model. To make the code easier to read, it has been adapted for fine-tuning T5-small for question answering on SQuAD. However with small fixes this code is totally usable for any of the tasks performed in the paper."],"metadata":{"id":"kDcbXYDkRbe2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qfmwNCkCIU84"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","source":["!pip install --quiet transformers\n","!pip install --quiet nlp\n","!pip install --quiet tokenizers\n","!pip install --quiet datasets"],"metadata":{"id":"s7umCNerIyTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import transformers\n","import nlp\n","from datasets import load_dataset\n","from transformers import T5TokenizerFast as T5Tokenizer"],"metadata":{"id":"0CZAqEBqI3sr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Pre-processing\n","First it is loaded the dataset. In order to fine-tune on a task belonging to GLUE or SuperGLUE benchmarks, it also is needed to specify the name of the task, so that the related dataset is loaded correctly."],"metadata":{"id":"gk2WkdsRSV93"}},{"cell_type":"code","source":["train_dataset  = load_dataset('squad', split=\"train\")\n","valid_dataset = load_dataset('squad', split=\"validation\")"],"metadata":{"id":"LKx3rNqcKno5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For this example the prefix is directly specified into the *add_eos_to_example* function. However it is possible to specify the prefix here and later properly add it."],"metadata":{"id":"VThkLxTXTNB-"}},{"cell_type":"code","source":["prefix = \"\"\n","max_input_length = 512\n","max_target_length = 16"],"metadata":{"id":"rIAgf_WbJ51p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The example inputs are pre-processed by adding the prefix, elaborating the target text format and inserting the eos (**e**nd **o**f **s**entence) token."],"metadata":{"id":"KWnfXlW_T3KD"}},{"cell_type":"code","source":["def add_eos_to_examples(example):\n","    example['input_text'] = 'question: %s  context: %s </s>' % (example['question'], example['context'])\n","    example['target_text'] = '%s </s>' % example['answers']['text'][0]\n","    return example"],"metadata":{"id":"UiPy5XjvKQ90"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the tokenizer."],"metadata":{"id":"bGVg_VW5Uvcz"}},{"cell_type":"code","source":["tokenizer = T5Tokenizer.from_pretrained('t5-small')"],"metadata":{"id":"6NxDRI_ALfq2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then the examples are encoded through the tokenizer. This function builds the encodings."],"metadata":{"id":"p5DUOTVRUVn_"}},{"cell_type":"code","source":["def convert_to_features(example_batch):\n","    input_encodings = tokenizer.batch_encode_plus(example_batch['input_text'], pad_to_max_length=True, max_length=512)\n","    target_encodings = tokenizer.batch_encode_plus(example_batch['target_text'], pad_to_max_length=True, max_length=16)\n","\n","    encodings = {\n","        'input_ids': input_encodings['input_ids'], \n","        'attention_mask': input_encodings['attention_mask'],\n","        'labels': target_encodings['input_ids'],\n","        'decoder_attention_mask': target_encodings['attention_mask']\n","    }\n","\n","    return encodings"],"metadata":{"id":"zmC1fy1vKiyR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally the dataset is mapped accordingly leveraging the previous functions."],"metadata":{"id":"2SHluNxvUzl2"}},{"cell_type":"code","source":["train_dataset = train_dataset.map(add_eos_to_examples)\n","train_dataset = train_dataset.map(convert_to_features, batched=True)\n","valid_dataset = valid_dataset.map(add_eos_to_examples, load_from_cache_file=False)\n","valid_dataset = valid_dataset.map(convert_to_features, batched=True, load_from_cache_file=False)"],"metadata":{"id":"hN7g_5icKv_A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Remove unused columns."],"metadata":{"id":"ZUFU4MQMVMWL"}},{"cell_type":"code","source":["columns = ['input_ids', 'labels', 'attention_mask', 'decoder_attention_mask']\n","train_dataset.set_format(type='torch', columns=columns)\n","valid_dataset.set_format(type='torch', columns=columns)"],"metadata":{"id":"ZEBtBZcJK30_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Save the datasets: it will be possible to load them directly during training."],"metadata":{"id":"0k8wYwUjVTU8"}},{"cell_type":"code","source":["torch.save(train_dataset, 'train_data.pt')\n","torch.save(validation_dataset, 'valid_data.pt')"],"metadata":{"id":"2sM_3x3uMGGx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"9ecMPdmxViQz"}},{"cell_type":"code","source":["import dataclasses\n","import logging\n","import os\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Dict, List, Optional\n","\n","import numpy as np\n","import torch\n","import torch.optim\n","import tensorflow as tf\n","import datetime\n","\n","from transformers import T5ForConditionalGeneration, T5TokenizerFast as T5Tokenizer, EvalPrediction\n","from transformers import (\n","    HfArgumentParser,\n","    DataCollator,\n","    Trainer,\n","    TrainingArguments,\n","    set_seed,\n",")\n","from transformers import integrations"],"metadata":{"id":"Ut1ldiIfLLOL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logger = logging.getLogger(__name__)"],"metadata":{"id":"LqIzHRtRLNXh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build a DataCollator. It takes a list of sample from a Dataset and collates them into a batch. It returns a dictionary of tensors with the keys that the forward method is expecting to receive. This is necessary because the Trainer passes directly this dictionary to the model as argument."],"metadata":{"id":"qHnYRFvXVpnm"}},{"cell_type":"code","source":["@dataclass\n","class T2TDataCollator:\n","\n","    def __call__(self, batch: List) -> Dict[str, torch.Tensor]:\n","\n","        input_ids = torch.stack([example['input_ids'] for example in batch])\n","        labels = torch.stack([example['labels'] for example in batch])\n","        labels[labels[:, :] == 0] = -100\n","        attention_mask = torch.stack([example['attention_mask'] for example in batch])\n","        decoder_attention_mask = torch.stack([example['decoder_attention_mask'] for example in batch])\n","        \n","        return {\n","            'input_ids': input_ids, \n","            'attention_mask': attention_mask,\n","            'labels': labels, \n","            'decoder_attention_mask': decoder_attention_mask\n","        }"],"metadata":{"id":"9BNbFFQtLPb7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Arguments pertaining to which model/config/tokenizer we are going to fine-tune from."],"metadata":{"id":"GDFkvY49WKa3"}},{"cell_type":"code","source":["@dataclass\n","class ModelArguments:\n","\n","    model_name_or_path: str = field(\n","        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n","    )"],"metadata":{"id":"jNwH4KkpLv5Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Arguments pertaining to what data we are going to input our model for training and eval."],"metadata":{"id":"teMdH7xFWUXS"}},{"cell_type":"code","source":["@dataclass\n","class DataTrainingArguments:\n","\n","    train_file_path: Optional[str] = field(\n","        default='train_data.pt',\n","        metadata={\"help\": \"Path for cached train dataset\"},\n","    )\n","    valid_file_path: Optional[str] = field(\n","        default='valid_data.pt',\n","        metadata={\"help\": \"Path for cached valid dataset\"},\n","    )\n","    max_len: Optional[int] = field(\n","        default=max_input_length,\n","        metadata={\"help\": \"Max input length for the source text\"},\n","    )\n","    target_max_len: Optional[int] = field(\n","        default=max_target_length,\n","        metadata={\"help\": \"Max input length for the target text\"},\n","    )"],"metadata":{"id":"G1m2-JlfL1ak"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Main function which contains the code to effectively fine-tune the pre-trained model."],"metadata":{"id":"8CM4mQgzWWm9"}},{"cell_type":"code","source":["def main():\n","\n","    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n","\n","    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath('args.json'))\n","\n","    if (\n","        os.path.exists(training_args.output_dir)\n","        and os.listdir(training_args.output_dir)\n","        and training_args.do_train\n","        and not training_args.overwrite_output_dir\n","    ):\n","        raise ValueError(\n","            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n","        )\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.info(\"Training/evaluation parameters %s\", training_args)\n","\n","    # Set seed\n","    set_seed(training_args.seed)\n","\n","    # Load pretrained model\n","    model = T5ForConditionalGeneration.from_pretrained(\n","        model_args.model_name_or_path,\n","        cache_dir=model_args.cache_dir,\n","    )\n","\n","    # Get datasets\n","    print('Loading data...')\n","    train_dataset  = torch.load(data_args.train_file_path)\n","    valid_dataset = torch.load(data_args.valid_file_path)\n","    print('Loading done!')\n","\n","    # Initialize the Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=valid_dataset,\n","        data_collator=T2TDataCollator(),\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        trainer.train(\n","            resume_from_checkpoint=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n","        )\n","        trainer.save_model()\n","\n","    # Evaluation\n","    results = {}\n","    if training_args.do_eval and training_args.local_rank in [-1, 0]:\n","        logger.info(\"*** Evaluate ***\")\n","\n","        eval_output = trainer.evaluate()\n","\n","        logger.info(\"***** Eval results *****\")\n","          for key in sorted(eval_output.keys()):\n","              logger.info(\"  %s = %s\", key, str(eval_output[key]))\n","              results.update(eval_output)\n","    \n","    return results"],"metadata":{"id":"POP6JQ7PMNTo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load TensorBoard to monitor the training performance. For fine-tuning sake this step is not necessary, but always useful."],"metadata":{"id":"VL6cf-ULWnNE"}},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"FIEBQqpSMwvQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tensorboard --logdir ./models/gpu/runs"],"metadata":{"id":"KwbINLjWMyMw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json"],"metadata":{"id":"dYppwnvgM0_L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These are the TrainingArguments used by the model at training-time."],"metadata":{"id":"lgNrxQ4UW2Y2"}},{"cell_type":"code","source":["args_dict = {\n","  \"model_name_or_path\": 't5-small',\n","  \"max_len\": max_input_length ,\n","  \"target_max_len\": max_target_length,\n","  \"output_dir\": './models/gpu',\n","  \"overwrite_output_dir\": True,\n","  \"per_device_train_batch_size\": 32,\n","  \"per_device_eval_batch_size\": 32,\n","  \"learning_rate\": 1e-4,\n","  \"num_train_epochs\": 5,\n","  \"optim\":\"adafactor\",\n","  \"do_train\": True,\n","  \"logging_steps\": 500,\n","  \"logging_first_step\": True,\n","  \"save_steps\": 500,\n","  \"do_eval\": True,\n","  \"evaluation_strategy\": \"steps\",\n","  \"eval_steps\": 500,\n","}"],"metadata":{"id":"hafJLQQxM2Yu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('args.json', 'w') as f:\n","  json.dump(args_dict, f)"],"metadata":{"id":"n48drfcqNN1L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main()"],"metadata":{"id":"KFkmqnhaNLyL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation\n","After having fine-tuned the model, it can be used to generate the predictions. As stated in the T5 paper, since the SQuAD test dataset coincides with the validation dataset, here is reported the evaluation process. However, the GLUE and SuperGLUE tasks have to be evaluated on the related servers."],"metadata":{"id":"8EyKGl4kXCw9"}},{"cell_type":"code","source":["import nlp\n","import pickle\n","from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","from tqdm.auto import tqdm\n","import argparse\n","import glob\n","import os\n","import json\n","import time\n","import logging\n","import random\n","import re\n","from itertools import chain\n","from string import punctuation\n","from sklearn import metrics\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader"],"metadata":{"id":"FccHnX9MQjvs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the fine-tuned model."],"metadata":{"id":"JYpjfD3OX3TQ"}},{"cell_type":"code","source":["model = T5ForConditionalGeneration.from_pretrained('/content/models/gpu/')"],"metadata":{"id":"vBRTq5woQko8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is the dataset for the evaluation."],"metadata":{"id":"P4Xc5jJCX5zT"}},{"cell_type":"code","source":["test_dataset = torch.load('valid_data.pt')\n","dataloader = DataLoader(test_dataset, batch_size=32)"],"metadata":{"id":"Vc74rPkuQv3q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generate the predictions."],"metadata":{"id":"dyOB6CtiX-Mz"}},{"cell_type":"code","source":["answers = []\n","for batch in tqdm(dataloader):\n","  outs = model.generate(input_ids=batch['input_ids'], \n","                        attention_mask=batch['attention_mask'],\n","                        max_length=16)\n","  outs = [tokenizer.decode(ids) for ids in outs]\n","  answers.extend(outs)"],"metadata":{"id":"HEBPRuypQ42T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Retrieve the predictions (with some clean-up):"],"metadata":{"id":"3Cu9LxdRYEz-"}},{"cell_type":"code","source":["predictions = []\n","for preds in answers:\n","  pred = preds.replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n","  predictions.append(pred[1:])"],"metadata":{"id":"hJpwj80-RCIA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And these are the references, i.e. the ground truth."],"metadata":{"id":"KpIM85_7YKkk"}},{"cell_type":"code","source":["references = []\n","for refs in valid_dataset[\"answers\"]:\n","  references.append(refs[\"text\"])"],"metadata":{"id":"SwJRYZYiRD4K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The last step for evaluating our fine-tuned model is to validate the *predictions* and the *references* using the metric referring to the task performed."],"metadata":{"id":"t3VDGbs3YRO0"}}]}